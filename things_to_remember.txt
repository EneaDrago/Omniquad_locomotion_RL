Lavora sempre nel conda environment in cui hai installato isaacsim: conda activate env_isaaclab

# Lanciare addestramento
1) Innanzitutto, preparare il file velocity_env_omni_cfg_vx.py, con tutte le specifiche del training che vuoi fare.
    Questi files devono essere inseriti qui: /home/<nomeutente>/IsaacLab/source/isaaclab_tasks/isaaclab_tasks/manager_based/locomotion/velocity/velocity_env_omni_cfg_v9.py
2) Metti la versione giusta del file creato al punto 1 nel file /home/<nomeutente>/IsaacLab/source/isaaclab_tasks/isaaclab_tasks/manager_based/locomotion/velocity/config/omniquad/rough_env_cfg.py
3) Aggiungi un nuovo "gym.register" (per farlo copia e incolla uno di quelli già presenti. Stai attento a selezionare la giusta opzione tra flat e rough!) nel file:
    /home/simone/IsaacLab/source/isaaclab_tasks/isaaclab_tasks/manager_based/locomotion/velocity/config/omniquad/__init__.py
4) Infine, sei pronto a lanciare il training. Per farlo, usa questi comandi:
    # Lanciare addestramento Flat
      ./isaaclab.sh -p scripts/reinforcement_learning/rsl_rl/train.py --task=Isaac-Velocity-Flat-OmniQuad-v6 --headless
    # Lanciare addestramento Rough
      ./isaaclab.sh -p scripts/reinforcement_learning/rl_games/train.py --task=Isaac-Velocity-Rough-OmniQuad-v2 --headless
    NOTA: devi mettere nell'argomento --task il nome dato al gym.register

    
# Il codice si trova qui:
source/isaaclab_tasks/isaaclab_tasks/manager_based/locomotion/velocity/config/omniquad/init.py

# Lanciare visualizzazione ultimo checkpoint
./isaaclab.sh -p scripts/reinforcement_learning/rsl_rl/play.py --task=Isaac-Velocity-Flat-OmniQuad-v0 --num_envs 32 --resume
./isaaclab.sh -p scripts/reinforcement_learning/rl_games/play.py --task=Isaac-Velocity-Rough-OmniQuad-v0 --num_envs 32 --use_last_checkpoint


# Aprire Tensorboard
./isaaclab.sh -p -m tensorboard.main --logdir logs/rsl_rl/omniquad_flat/

# Se fai casino con github
git fetch origin
git reset --hard origin/dev1



# FILE IMPORTANTI: 
Vai dentro source/isaaclab_tasks/isaaclab_tasks/manager_based/locomotion/velocity/
- dentro config/agents ci sono i parametri dell'addestramento (es. num di epochs,etc. )  --> questo non toccarlo perché funziona bene da sé
- Dentro la cartella config c'è il file velocity_env_omni_cfg.py, che crea tutte le classi per l'addestramento (es.rewards, observations, etc.)
- Dentro la cartella config/mdp (Markovian Decision Process), ci sono i file curriculums.py, rewards.py e terminations.py --> di base, questo non serve modificarlo, a meno che tu voglia aggiungere altri tipi di reward
Vai dentro \source\isaaclab_assets\isaaclab_assets\robots\
- Nel file omniquad.py, c'è la configurazione del robot (es. quanti e attuatori, etc.)

##===================================================================================
# TEMPORANEO:
ho creato 3 versioni di velocity_env_omni_cfg:  default --> quella di Francesco
                                                v1      --> aggiunta feet air time
                                                v2      --> aggiunta joint deviation

per scegliere quella con cui fare l'addestramento andare dentro rough_env_cfg e nell'importare la versione con la quale si vuole fare il training

NOTA: è un implementazione veloce da usare come supporto al ricevimento con Francesco, ==> da modificare assolutamente
##===================================================================================


TRANSFER LEARNING:
Per far cominciare l'addestramento da un checkpoint precedente, aprire la cartella logs e identificare il checkpoint 
di interesse (ad es., logs/rsl_rl/omniquad_flat/2025-07-05_18-49-55/model_299.pt)
Dopodiché, lanciare il comando del training aggiungendo l'argomento "checkpoint_path":
./isaaclab.sh -p scripts/reinforcement_learning/rl_games/train.py \
  --task=Isaac-Velocity-Rough-OmniQuad-v1 \
  --resume \
  --checkpoint_path=logs/rsl_rl/omniquad_flat/2025-07-05_18-49-55/model_299.pt