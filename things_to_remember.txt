
# Lanciare addestramento Flat
./isaaclab.sh -p scripts/reinforcement_learning/rsl_rl/train.py --task=Isaac-Velocity-Flat-OmniQuad-v0 --headless

# Il codice si trova qui:
source/isaaclab_tasks/isaaclab_tasks/manager_based/locomotion/velocity/config/omniquad/init.py

# Lanciare visualizzazione ultimo checkpoint
./isaaclab.sh -p scripts/reinforcement_learning/rsl_rl/play.py --task=Isaac-Velocity-Flat-OmniQuad-v0 --num_envs 32 --resume


# Lanciare addestramento Rough
./isaaclab.sh -p scripts/reinforcement_learning/rsl_rl/train.py --task=Isaac-Velocity-Rough-OmniQuad-v0 --headless --max_iterations=300


git fetch origin
git reset --hard origin/dev1



# FILE IMPORTANTI: 
Vai dentro source/isaaclab_tasks/isaaclab_tasks/manager_based/locomotion/velocity/
- dentro config/agents ci sono i parametri dell'addestramento (es. num di epochs,etc. )  --> questo non toccarlo perché funziona bene da sé
- Dentro la cartella config c'è il file velocity_env_omni_cfg.py, che crea tutte le classi per l'addestramento (es.rewards, observations, etc.)
- Dentro la cartella config/mdp (Markovian Decision Process), ci sono i file curriculums.py, rewards.py e terminations.py --> di base, questo non serve modificarlo, a meno che tu voglia aggiungere altri tipi di reward
Vai dentro \source\isaaclab_assets\isaaclab_assets\robots\
- Nel file omniquad.py, c'è la configurazione del robot (es. quanti e attuatori, etc.)